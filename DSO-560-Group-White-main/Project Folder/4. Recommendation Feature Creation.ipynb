{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Recommendation Feature Creation<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Define-Functions\" data-toc-modified-id=\"Define-Functions-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Define Functions</a></span></li><li><span><a href=\"#Creating-Features\" data-toc-modified-id=\"Creating-Features-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Creating Features</a></span><ul class=\"toc-item\"><li><span><a href=\"#Recommendation-Token\" data-toc-modified-id=\"Recommendation-Token-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Recommendation Token</a></span><ul class=\"toc-item\"><li><span><a href=\"#Patterns\" data-toc-modified-id=\"Patterns-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Patterns</a></span></li><li><span><a href=\"#Extract-Patterns\" data-toc-modified-id=\"Extract-Patterns-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Extract Patterns</a></span></li></ul></li><li><span><a href=\"#POS-Token\" data-toc-modified-id=\"POS-Token-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>POS Token</a></span></li><li><span><a href=\"#Brand-Token\" data-toc-modified-id=\"Brand-Token-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Brand Token</a></span></li><li><span><a href=\"#Merge-feature-for-initial-product\" data-toc-modified-id=\"Merge-feature-for-initial-product-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Merge feature for initial product</a></span></li><li><span><a href=\"#Final-Tags\" data-toc-modified-id=\"Final-Tags-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Final Tags</a></span></li><li><span><a href=\"#Lemmatization-&amp;-Remove-Stopwords\" data-toc-modified-id=\"Lemmatization-&amp;-Remove-Stopwords-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Lemmatization &amp; Remove Stopwords</a></span></li><li><span><a href=\"#Remove-Special-Words\" data-toc-modified-id=\"Remove-Special-Words-2.7\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;</span>Remove Special Words</a></span></li><li><span><a href=\"#Drop-Duplicate-words-in-Recommendation-doc\" data-toc-modified-id=\"Drop-Duplicate-words-in-Recommendation-doc-2.8\"><span class=\"toc-item-num\">2.8&nbsp;&nbsp;</span>Drop Duplicate words in Recommendation doc</a></span></li><li><span><a href=\"#Remove-Extra-Space-and-Output-file\" data-toc-modified-id=\"Remove-Extra-Space-and-Output-file-2.9\"><span class=\"toc-item-num\">2.9&nbsp;&nbsp;</span>Remove Extra Space and Output file</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T03:50:36.493918Z",
     "start_time": "2021-05-11T03:50:25.003467Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T03:50:55.451831Z",
     "start_time": "2021-05-11T03:50:36.496494Z"
    }
   },
   "outputs": [],
   "source": [
    "product = pd.read_excel('Behold+product+data+04262021.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T03:50:55.517041Z",
     "start_time": "2021-05-11T03:50:55.460418Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def remove_n(col):\n",
    "    '''Remove change line character'''\n",
    "    return col.fillna(' ').astype(str).apply(lambda x: re.sub('\\n',' ',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T03:50:55.535557Z",
     "start_time": "2021-05-11T03:50:55.520019Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def remove_comma(col):\n",
    "    '''Replace comma with space'''\n",
    "    return col.apply(lambda x: re.sub(',',' ',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T03:50:55.585667Z",
     "start_time": "2021-05-11T03:50:55.575055Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def decomposite_list(col):\n",
    "    '''decomposite list into string'''\n",
    "    return col.apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T03:50:55.613557Z",
     "start_time": "2021-05-11T03:50:55.594007Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    '''function to convert nltk tag to wordnet tag'''\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T03:50:55.645673Z",
     "start_time": "2021-05-11T03:50:55.616757Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def lemmatize_sentence(sentence):\n",
    "    '''Use NLTK package to perform lemmatize'''\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(word_tokenize(sentence))  \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return ' '.join(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T03:50:55.661053Z",
     "start_time": "2021-05-11T03:50:55.651003Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def remove_sw(text):\n",
    "    '''Remove stop words'''\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk import word_tokenize\n",
    "\n",
    "    stopwords_list = stopwords.words('english')\n",
    "\n",
    "    import re\n",
    "\n",
    "    # split sentence into words\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    new_words = []\n",
    "    # remove stopwords\n",
    "    for w in words:\n",
    "        if w in stopwords_list:\n",
    "            continue\n",
    "        new_words.append(w)\n",
    "\n",
    "    return ' '.join(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T03:50:55.674199Z",
     "start_time": "2021-05-11T03:50:55.666605Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def textClean(text):   \n",
    "    '''Function combines lemmatization and stopwords removal'''\n",
    "    return(remove_sw(lemmatize_sentence(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T03:50:55.690438Z",
     "start_time": "2021-05-11T03:50:55.678546Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def product_POS(text):\n",
    "    '''Use nlp to extract pattern of speech for product features, which extract both noun and adjectives'''\n",
    "    word = []\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if (token.pos_ == 'ADJ') | (token.pos_ == 'NOUN') :\n",
    "            word.append(token.text)\n",
    "    return word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T03:50:55.710010Z",
     "start_time": "2021-05-11T03:50:55.699987Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def recom_POS(text):\n",
    "    '''Use nlp to extract pattern of speech for recommendation features, which extract only adjectives'''\n",
    "    word = []\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if (token.pos_ == 'ADJ'):\n",
    "            word.append(token.text)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T03:50:55.724038Z",
     "start_time": "2021-05-11T03:50:55.715609Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def removeExtraSpace(doc):\n",
    "    '''Remove extra spaces'''\n",
    "    cleaned_list = []\n",
    "    for text in doc.split():\n",
    "        cleaned_list.append(text.strip())\n",
    "    return ' '.join(cleaned_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T03:50:55.736276Z",
     "start_time": "2021-05-11T03:50:55.728450Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def remove_special_word(col):\n",
    "    '''Remove special word'''\n",
    "    return col.apply(lambda x: re.sub(r'[^A-Za-z0-9%_ ]+', '',x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preproceesing description and details, remove '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T03:50:56.300192Z",
     "start_time": "2021-05-11T03:50:55.739524Z"
    }
   },
   "outputs": [],
   "source": [
    "product['description'] = remove_n(product['description'])\n",
    "product['details'] = remove_n(product['details'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Recommendation Token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- We analyzed the description and details columns, and figured out some patterns that implies the potential outfit recommendation styles and combinations, which includes with, allow for, style tip, pair, under, wear over, tuck in, and continue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T03:50:56.312539Z",
     "start_time": "2021-05-11T03:50:56.303607Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create regex patterns for recommendation tokens\n",
    "\n",
    "with_pattern = r'([\\w -,]+?(?:with\\b)[\\w ,]+)'\n",
    "for_pattern = r'([\\w -,]+?(?:allow\\b|allows\\b)?:[\\w ,]+(?:\\bfor\\b)[\\w ,]+)'\n",
    "style_pattern = r'([\\w -,]+?(?:style\\b|Style\\b)?:[\\w ,]+(?:\\bTip\\b|\\btip\\b|trick\\b)[\\w ,]+)' \n",
    "pair_pattern = r'([\\w -,]+?(?:pair)[\\w ,]+)' \n",
    "under_pattern = r'([\\w -,]+?(?:under\\b)[\\w ,]+)'\n",
    "over_pattern = r'([\\w -,]+?(?:wear|worn)[\\w ,]+?(?:over\\b)[\\w ,]+)' \n",
    "tuck_in_pattern = r'([\\w -,]+?(?:\\btuck\\b)[\\w ,]+?(?:\\bin\\b)[\\w ,]+)'\n",
    "continue_pattern = r'([\\w -,]+?(?:\\bcontinu)[\\w ,]+)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Extract Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T03:55:42.494609Z",
     "start_time": "2021-05-11T03:50:56.315708Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## extract patterns from Description column\n",
    "product['With Pattern'] = product['description'].astype(str).apply(lambda x: re.findall(with_pattern, x))\n",
    "product['Continue Pattern'] = product['description'].astype(str).apply(lambda x: re.findall(continue_pattern, x))\n",
    "product['Tuck Pattern'] = product['description'].astype(str).apply(lambda x: re.findall(tuck_in_pattern, x))\n",
    "product['Over Pattern'] = product['description'].astype(str).apply(lambda x: re.findall(over_pattern, x))\n",
    "product['Under Pattern'] = product['description'].astype(str).apply(lambda x: re.findall(under_pattern, x))\n",
    "product['Pair Pattern'] = product['description'].astype(str).apply(lambda x: re.findall(pair_pattern, x))\n",
    "product['Style Pattern'] = product['description'].astype(str).apply(lambda x: re.findall(style_pattern, x))\n",
    "product['With Pattern'] = product['description'].astype(str).apply(lambda x: re.findall(with_pattern, x))\n",
    "product['For Pattern']=product['description'].astype(str).apply(lambda x: re.findall(for_pattern, x))\n",
    "\n",
    "## extract patterns from Details column\n",
    "product['With Pattern2'] = product['details'].astype(str).apply(lambda x: re.findall(with_pattern, x))\n",
    "product['Continue Pattern2'] = product['details'].astype(str).apply(lambda x: re.findall(continue_pattern, x))\n",
    "product['Tuck Pattern2'] = product['details'].astype(str).apply(lambda x: re.findall(tuck_in_pattern, x))\n",
    "product['Over Pattern2'] = product['details'].astype(str).apply(lambda x: re.findall(over_pattern, x))\n",
    "product['Under Pattern2'] = product['details'].astype(str).apply(lambda x: re.findall(under_pattern, x))\n",
    "product['Pair Pattern2'] = product['details'].astype(str).apply(lambda x: re.findall(pair_pattern, x))\n",
    "product['Style Pattern2'] = product['details'].astype(str).apply(lambda x: re.findall(style_pattern, x))\n",
    "product['With Pattern2'] = product['details'].astype(str).apply(lambda x: re.findall(with_pattern, x))\n",
    "product['For Pattern2']=product['details'].astype(str).apply(lambda x: re.findall(for_pattern, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T03:55:43.861067Z",
     "start_time": "2021-05-11T03:55:42.496689Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## combine the extracted patterns and remove signal words (e.g. with)\n",
    "\n",
    "product['Recommendation pattern'] = product['Continue Pattern']+product['Tuck Pattern']+product['Over Pattern']+product['Under Pattern']+product['Pair Pattern']+product['Style Pattern']+product['With Pattern']+product['For Pattern']+product['Continue Pattern2']+product['Tuck Pattern2']+product['Over Pattern2']+product['Under Pattern2']+product['Pair Pattern2']+product['Style Pattern2']+product['With Pattern2']+product['For Pattern2']\n",
    "product['Recommendation pattern'] = decomposite_list(product['Recommendation pattern'])\n",
    "product['Recommendation pattern'] = product['Recommendation pattern'].apply(lambda x: re.sub('with|allow|allows|for|under|wear|over|continue',' ',x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## POS Token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Here we extract noun and adjectives from details and description to add into product feature\n",
    "- Extract adjectives from details and description to add into recommendation feature: the overfit that the user is looking for may have large overlapping in adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T04:47:26.992754Z",
     "start_time": "2021-05-11T03:55:43.863229Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "product['POS_product1'] = product['description'].apply(product_POS)\n",
    "product['POS_product2'] = product['details'].apply(product_POS)\n",
    "\n",
    "product['POS_recom1'] = product['description'].apply(recom_POS)\n",
    "product['POS_recom2'] = product['details'].apply(recom_POS)\n",
    "\n",
    "product['name'] = remove_n(product['name'])\n",
    "product['POS_name']=product['name'].fillna(' ').astype(str).apply(recom_POS)\n",
    "\n",
    "product['POS_product'] = product['POS_product1']+product['POS_product2']\n",
    "product['POS_recom'] = product['POS_recom1']+product['POS_recom2']+product['POS_name']\n",
    "\n",
    "\n",
    "product['POS_product'] = decomposite_list(product['POS_product'])\n",
    "product['POS_recom'] = decomposite_list(product['POS_recom'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Brand Token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- To avoid stopword removal in brand, we created a single brand token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T04:50:11.770642Z",
     "start_time": "2021-05-11T04:47:26.996315Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for j in range(len(product)):\n",
    "    a='BRAND_'\n",
    "    if type(product.loc[j,'brand'])==int:\n",
    "        product.loc[j,'brand'] = str(product.loc[j,'brand'])\n",
    "    for i in product.loc[j,'brand'].split(' '):\n",
    "        a+=i\n",
    "    a+='_TOKEN'\n",
    "    product.loc[j,'Brand_token']=a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T04:50:11.905336Z",
     "start_time": "2021-05-11T04:50:11.773683Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "product['brand_token'] = product['brand'].astype(str).apply(lambda x: re.sub('\\W',' ',x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Merge feature for initial product "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Import cleaned data from feature extraction and merge useful features to a single token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T04:50:12.938409Z",
     "start_time": "2021-05-11T04:50:11.907598Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "feature_df = pd.read_csv('all_features.csv')\n",
    "feature_df = feature_df.fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Final Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Select the target columns that will be used in similarity calculation and recommendation providing\n",
    "- Combine all the target columns into one dataframe called 'final'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T04:50:13.099144Z",
     "start_time": "2021-05-11T04:50:12.940606Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "final=product[['product_id','brand','name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T04:50:13.117802Z",
     "start_time": "2021-05-11T04:50:13.102132Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "final['general_category']=feature_df['general_category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T04:50:13.406271Z",
     "start_time": "2021-05-11T04:50:13.120084Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "final['recommendation_doc']=product['brand_token']+' '+product['POS_recom']+' '+product['Recommendation pattern']+' '+feature_df['style']+' '+feature_df['pattern']+' '+feature_df['color']+' '+feature_df['occasion']+' '+feature_df['material']+' '+feature_df['material_percent']+' '+feature_df['trend']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T04:50:13.632980Z",
     "start_time": "2021-05-11T04:50:13.409440Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "final['product_doc']=product['name']+' '+product['POS_product']+' '+product['brand_token']+' '+product['Brand_token']+' '+feature_df['all_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T04:50:13.830175Z",
     "start_time": "2021-05-11T04:50:13.635505Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "final['recommendation_doc'] = remove_comma(final['recommendation_doc'])\n",
    "final['product_doc'] = remove_comma(final['product_doc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Lemmatization & Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T05:01:53.052977Z",
     "start_time": "2021-05-11T04:50:13.832473Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "final[\"product_doc\"] =final[\"product_doc\"].apply(textClean)\n",
    "final[\"recommendation_doc\"] =final[\"recommendation_doc\"].apply(textClean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Remove Special Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T05:01:54.526968Z",
     "start_time": "2021-05-11T05:01:53.056939Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "final['product_doc'] = remove_special_word(final['product_doc'])\n",
    "final['recommendation_doc'] = remove_special_word(final['recommendation_doc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Drop Duplicate words in Recommendation doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T05:02:12.777149Z",
     "start_time": "2021-05-11T05:01:54.530018Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(final)):\n",
    "    words=final.loc[i,'recommendation_doc'].split()\n",
    "    final.loc[i,'recommendation_doc'] = \" \".join(sorted(set(words), key=words.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T05:02:12.976622Z",
     "start_time": "2021-05-11T05:02:12.780496Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "final['product_doc'] = final['product_doc'].apply(lambda x: re.sub('unknown','',x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Remove Extra Space and Output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T05:02:14.774567Z",
     "start_time": "2021-05-11T05:02:12.987208Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "final['product_doc'] = final['product_doc'].apply(removeExtraSpace)\n",
    "final['recommendation_doc'] = final['recommendation_doc'].apply(removeExtraSpace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T05:03:17.052378Z",
     "start_time": "2021-05-11T05:03:15.541231Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# export the final feature csv\n",
    "final.to_csv('final_feature.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Recommendation Feature Creation",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
