{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Modeling<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Loading-in-dataset\" data-toc-modified-id=\"Loading-in-dataset-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Loading in dataset</a></span></li><li><span><a href=\"#Assigning-predictor-variable-and-target-variable\" data-toc-modified-id=\"Assigning-predictor-variable-and-target-variable-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Assigning predictor variable and target variable</a></span></li><li><span><a href=\"#Modeling---preprocessing\" data-toc-modified-id=\"Modeling---preprocessing-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Modeling - preprocessing</a></span></li><li><span><a href=\"#Modeling---cross-validation-and-performance-evaluation\" data-toc-modified-id=\"Modeling---cross-validation-and-performance-evaluation-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Modeling - cross-validation and performance evaluation</a></span></li><li><span><a href=\"#Modeling---final-model\" data-toc-modified-id=\"Modeling---final-model-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Modeling - final model</a></span></li><li><span><a href=\"#Evaluate-the-model-using-your-test-dataset\" data-toc-modified-id=\"Evaluate-the-model-using-your-test-dataset-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Evaluate the model using your test dataset</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: please load and preprocess your test dataset along with the original (training) dataset until the final model training part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk import punkt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Loading in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# load in the notebook that contains all extracted features we created for the products\n",
    "product = pd.read_csv('all_features.csv')\n",
    "\n",
    "# load in the original notebook for all products\n",
    "product_original = pd.read_excel('Behold+product+data+04262021.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# appending the brand of each product to the dataframe that has extracted features \n",
    "product['brand'] = product_original['brand']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The final dataframe contains lemmatized description of products (in one column), each of the 30 features that are extracted from the texts, all features combined (in one column), and the brand of each product\n",
    "\n",
    "Note: the 'lemm_total' column come from the 'brand_category','name','details','description' and 'brand_name' columns from the original product dataset. We included them because they carry helpful information for predicting the brand. Also note that even though 'brand_category', 'name' and 'brand_name' sound like they are relevant to the brand (the target variable), the texts under these columns actually don't contain any words directly from the brand.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>lemm_total</th>\n",
       "      <th>detailed_category</th>\n",
       "      <th>general_category</th>\n",
       "      <th>gender</th>\n",
       "      <th>season</th>\n",
       "      <th>class</th>\n",
       "      <th>closure</th>\n",
       "      <th>color</th>\n",
       "      <th>dry_clean_only</th>\n",
       "      <th>...</th>\n",
       "      <th>toe_style</th>\n",
       "      <th>trend</th>\n",
       "      <th>wash</th>\n",
       "      <th>width</th>\n",
       "      <th>location</th>\n",
       "      <th>material_percent</th>\n",
       "      <th>material</th>\n",
       "      <th>brand_specific</th>\n",
       "      <th>all_features</th>\n",
       "      <th>brand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01EX0PN4J9WRNZH5F93YEX6QAF</td>\n",
       "      <td>unknown khadi stripe shirt our signature shirt...</td>\n",
       "      <td>shirt</td>\n",
       "      <td>top</td>\n",
       "      <td>NaN</td>\n",
       "      <td>spring</td>\n",
       "      <td>shirt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>black white</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>black white</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>shirt top  spring shirt  black white         ...</td>\n",
       "      <td>Two</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01F0C4SKZV6YXS3265JMC39NXW</td>\n",
       "      <td>unknown ruffle market dress loopy pink sistine...</td>\n",
       "      <td>dress</td>\n",
       "      <td>onepiece</td>\n",
       "      <td>woman</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dress</td>\n",
       "      <td>zipper strap</td>\n",
       "      <td>pink</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ny</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>organic</td>\n",
       "      <td>dress onepiece woman  dress zipper strap pink...</td>\n",
       "      <td>Collina Strada</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   product_id  \\\n",
       "0  01EX0PN4J9WRNZH5F93YEX6QAF   \n",
       "1  01F0C4SKZV6YXS3265JMC39NXW   \n",
       "\n",
       "                                          lemm_total detailed_category  \\\n",
       "0  unknown khadi stripe shirt our signature shirt...             shirt   \n",
       "1  unknown ruffle market dress loopy pink sistine...             dress   \n",
       "\n",
       "  general_category gender  season  class       closure        color  \\\n",
       "0              top    NaN  spring  shirt           NaN  black white   \n",
       "1         onepiece  woman     NaN  dress  zipper strap         pink   \n",
       "\n",
       "  dry_clean_only  ... toe_style trend         wash width location  \\\n",
       "0            NaN  ...       NaN   NaN  black white   NaN      NaN   \n",
       "1            NaN  ...       NaN   NaN          NaN   NaN       ny   \n",
       "\n",
       "  material_percent material brand_specific  \\\n",
       "0              NaN      NaN            NaN   \n",
       "1              NaN      NaN        organic   \n",
       "\n",
       "                                        all_features           brand  \n",
       "0   shirt top  spring shirt  black white         ...             Two  \n",
       "1   dress onepiece woman  dress zipper strap pink...  Collina Strada  \n",
       "\n",
       "[2 rows x 34 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Assigning predictor variable and target variable\n",
    "\n",
    "- For the predictor variable, we chose to combine all extracted features, and append it with lemmatized product descriptions&details in case there are not many features extracted. In this way, we can make sure that most products' predictor variable will have more than 64 words when we later pad the documents with max length 64. \n",
    "\n",
    "    \n",
    "- For the target variable, we chose to only include the top 30 appearing brands in the dataset as well as an 'other' category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# We use 'X' to denote the column that represents the predictor variable we are going to use in the model\n",
    "# It contains all the features of the product, followed by lemmatized description/details \n",
    "\n",
    "product['X'] = product['all_features'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# We use 'target' to denote the column that represents the target variable of the dataset, which contains a total\n",
    "# of 31 classes \n",
    "\n",
    "top30 = product.brand.value_counts()[:30].index.to_list()\n",
    "def assign_brand(name):\n",
    "    '''Assigns the brand to each record (either the top 30 brands or Other)'''\n",
    "    if name in top30:\n",
    "        return name\n",
    "    else:\n",
    "        return 'Other'\n",
    "product['target'] = product.brand.apply(assign_brand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Modeling - preprocessing\n",
    "\n",
    "- Remove stopwords for the texts in the predictor variable\n",
    "- Transform the predictor and target variable into the right formats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/BarbaraLiao/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/BarbaraLiao/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# importing nltk stopwords\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords') \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "english_stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "# adding 'unknown' as a stopword\n",
    "english_stopwords.add('unknown')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(title):\n",
    "    '''remove stopwords for a document'''\n",
    "    if isinstance(title, str):\n",
    "        tokens = nltk.word_tokenize(title)\n",
    "        filtered_tokens = []\n",
    "        for token in tokens:\n",
    "            if token in english_stopwords:\n",
    "                continue\n",
    "            filtered_tokens.append(token)\n",
    "            \n",
    "        return \" \".join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# removing stopwords for the predictor variable in the data set\n",
    "\n",
    "product[\"X\"] = product[\"X\"].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# formatting the predictor variable and target variable into 2 separate lists\n",
    "X = product['X'].to_list()\n",
    "Y = product['target'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Tokenize the top 5000 appearing words, and mark the rest as UNKNOWN_TOKEN\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token=\"UNKNOWN_TOKEN\")\n",
    "tokenizer.fit_on_texts(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[68, 57, 28, 25, 35]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "def integer_encode_documents(docs, tokenizer):\n",
    "    '''apply the input tokenizer on the input docs and return sequences'''\n",
    "    return tokenizer.texts_to_sequences(docs)\n",
    "\n",
    "# integer encode the documents\n",
    "encoded_docs = integer_encode_documents(X, tokenizer)\n",
    "# see some lengths of the documents\n",
    "list(map(len, encoded_docs))[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  14,   29,   92, ...,   26,  186,   26],\n",
       "       [   4,   56,   23, ...,    0,    0,    0],\n",
       "       [ 132,  218,   45, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [ 103,  105,  171, ..., 3604,  131, 3121],\n",
       "       [ 329,   44,  239, ...,  239,    1,   12],\n",
       "       [ 103,  105,  171, ..., 1938,    1,   12]], dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set MAX_SEQUENCE_LENGTH to 64\n",
    "MAX_SEQUENCE_LENGTH = 64\n",
    "\n",
    "# This is a list of lists, the numbers represent the index position of each word;\n",
    "# for instance, 33 means the 33rd word in the vocabulary\n",
    "\n",
    "# This step makes sure that each document in the predictor variable has a fixed length of 64 \n",
    "\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "padded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# This step encodes the 31 brand categories to 31 labels, and makes 31 binary columns for them\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "labels = to_categorical(encoder.fit_transform(Y),31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Modeling - cross-validation and performance evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<b> Note: please do not include your test dataset at this point<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# making 5-fold cross validation data sets\n",
    "# for each item in 'cv', it contains 4 lists that represent X_train, X_test,Y_train, and Y_test, respectively\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5,shuffle = True, random_state = 50)\n",
    "cv = []\n",
    "    \n",
    "for i, (train_index, test_index) in enumerate(kf.split(padded_docs, labels.argmax(1))):\n",
    "    X_train, X_test = padded_docs[train_index], padded_docs[test_index]\n",
    "    Y_train, Y_test = labels[train_index], labels[test_index]\n",
    "    cv += [[X_train, X_test,Y_train, Y_test]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from numpy import array, argmax, asarray, zeros\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = int(len(tokenizer.word_index) * 1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Reference: please download glove.6B.100d.txt from https://www.kaggle.com/danielwillgeorge/glove6b100dtxt\n",
    "\n",
    "def load_glove_vectors():\n",
    "    '''load in the glove vectors and return embeddings index'''\n",
    "    embeddings_index = {}\n",
    "    with open('glove.6B.100d.txt') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "    return embeddings_index\n",
    "\n",
    "\n",
    "embeddings_index = load_glove_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "\n",
    "embedding_matrix = zeros((VOCAB_SIZE, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: # check that it is an actual word that we have embeddings for\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# define lstm model\n",
    "\n",
    "import keras\n",
    "from keras.layers.recurrent import SimpleRNN, LSTM\n",
    "from keras.layers import Flatten, Masking\n",
    "\n",
    "def make_lstm_classification_model(plot=False):\n",
    "    '''output an lstm model'''\n",
    "    model =  keras.models.Sequential()\n",
    "    model.add(Embedding(VOCAB_SIZE, 100, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "    model.add(Masking(mask_value=0.0)) # masking layer, masks any words that don't have an embedding as 0s.\n",
    "    model.add(LSTM(units=32, input_shape=(1, MAX_SEQUENCE_LENGTH)))\n",
    "    model.add(Dense(16))\n",
    "    model.add(Dense(31, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # summarize the model\n",
    "    model.summary()\n",
    "    \n",
    "    if plot:\n",
    "        plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 64, 100)           3438900   \n",
      "_________________________________________________________________\n",
      "masking (Masking)            (None, 64, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 32)                17024     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 31)                527       \n",
      "=================================================================\n",
      "Total params: 3,456,979\n",
      "Trainable params: 18,079\n",
      "Non-trainable params: 3,438,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create an instance of the lstm model\n",
    "\n",
    "model = make_lstm_classification_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<b> Note: You can comment out the next 2 cells because it will take extremely long time to run <b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1381/1381 [==============================] - 33s 22ms/step - loss: 2.1453 - accuracy: 0.4583 - val_loss: 0.9627 - val_accuracy: 0.7403\n",
      "Epoch 2/20\n",
      "1381/1381 [==============================] - 29s 21ms/step - loss: 0.7563 - accuracy: 0.8046 - val_loss: 0.8975 - val_accuracy: 0.7399\n",
      "Epoch 3/20\n",
      "1381/1381 [==============================] - 30s 22ms/step - loss: 0.5189 - accuracy: 0.8635 - val_loss: 0.7834 - val_accuracy: 0.7853\n",
      "Epoch 4/20\n",
      "1381/1381 [==============================] - 31s 22ms/step - loss: 0.3955 - accuracy: 0.8977 - val_loss: 0.7407 - val_accuracy: 0.8048\n",
      "Epoch 5/20\n",
      "1381/1381 [==============================] - 28s 20ms/step - loss: 0.3246 - accuracy: 0.9135 - val_loss: 0.6276 - val_accuracy: 0.8262\n",
      "Epoch 6/20\n",
      "1381/1381 [==============================] - 28s 20ms/step - loss: 0.2737 - accuracy: 0.9285 - val_loss: 0.5765 - val_accuracy: 0.8519\n",
      "Epoch 7/20\n",
      "1381/1381 [==============================] - 29s 21ms/step - loss: 0.2377 - accuracy: 0.9372 - val_loss: 0.7528 - val_accuracy: 0.7959\n",
      "Epoch 8/20\n",
      "1381/1381 [==============================] - 27s 20ms/step - loss: 0.2160 - accuracy: 0.9403 - val_loss: 0.5991 - val_accuracy: 0.8539\n",
      "Epoch 9/20\n",
      "1381/1381 [==============================] - 29s 21ms/step - loss: 0.1982 - accuracy: 0.9474 - val_loss: 0.8038 - val_accuracy: 0.8067\n",
      "Epoch 10/20\n",
      "1381/1381 [==============================] - 27s 19ms/step - loss: 0.1737 - accuracy: 0.9526 - val_loss: 0.6856 - val_accuracy: 0.8376\n",
      "Epoch 11/20\n",
      "1381/1381 [==============================] - 27s 20ms/step - loss: 0.1595 - accuracy: 0.9580 - val_loss: 0.8023 - val_accuracy: 0.8244\n",
      "Epoch 12/20\n",
      "1381/1381 [==============================] - 27s 19ms/step - loss: 0.1496 - accuracy: 0.9599 - val_loss: 0.6868 - val_accuracy: 0.8513\n",
      "Epoch 13/20\n",
      "1381/1381 [==============================] - 27s 20ms/step - loss: 0.1360 - accuracy: 0.9649 - val_loss: 0.6193 - val_accuracy: 0.8692\n",
      "Epoch 14/20\n",
      "1381/1381 [==============================] - 28s 20ms/step - loss: 0.1336 - accuracy: 0.9646 - val_loss: 0.6963 - val_accuracy: 0.8466\n",
      "Epoch 15/20\n",
      "1381/1381 [==============================] - 27s 19ms/step - loss: 0.1189 - accuracy: 0.9682 - val_loss: 0.8071 - val_accuracy: 0.8205\n",
      "Epoch 16/20\n",
      "1381/1381 [==============================] - 27s 19ms/step - loss: 0.1154 - accuracy: 0.9689 - val_loss: 0.8213 - val_accuracy: 0.8350\n",
      "Epoch 17/20\n",
      "1381/1381 [==============================] - 27s 20ms/step - loss: 0.1079 - accuracy: 0.9705 - val_loss: 0.6988 - val_accuracy: 0.8607\n",
      "Epoch 18/20\n",
      "1381/1381 [==============================] - 28s 20ms/step - loss: 0.1029 - accuracy: 0.9735 - val_loss: 0.8183 - val_accuracy: 0.8340\n",
      "Epoch 19/20\n",
      "1381/1381 [==============================] - 27s 20ms/step - loss: 0.1031 - accuracy: 0.9723 - val_loss: 1.0976 - val_accuracy: 0.7926\n",
      "Epoch 20/20\n",
      "1381/1381 [==============================] - 28s 20ms/step - loss: 0.0931 - accuracy: 0.9752 - val_loss: 0.9712 - val_accuracy: 0.8264\n",
      "384/384 [==============================] - 2s 4ms/step - loss: 0.4028 - accuracy: 0.9232\n",
      "Epoch 1/20\n",
      "1381/1381 [==============================] - 27s 20ms/step - loss: 0.1536 - accuracy: 0.9626 - val_loss: 0.8448 - val_accuracy: 0.8336\n",
      "Epoch 2/20\n",
      "1381/1381 [==============================] - 28s 20ms/step - loss: 0.1257 - accuracy: 0.9684 - val_loss: 0.8171 - val_accuracy: 0.8460\n",
      "Epoch 3/20\n",
      "1381/1381 [==============================] - 28s 20ms/step - loss: 0.1160 - accuracy: 0.9698 - val_loss: 0.7897 - val_accuracy: 0.8509\n",
      "Epoch 4/20\n",
      "1381/1381 [==============================] - 27s 20ms/step - loss: 0.1066 - accuracy: 0.9728 - val_loss: 0.8786 - val_accuracy: 0.8216\n",
      "Epoch 5/20\n",
      "1381/1381 [==============================] - 27s 20ms/step - loss: 0.0990 - accuracy: 0.9746 - val_loss: 1.0090 - val_accuracy: 0.8099\n",
      "Epoch 6/20\n",
      "1381/1381 [==============================] - 27s 20ms/step - loss: 0.0975 - accuracy: 0.9746 - val_loss: 0.8954 - val_accuracy: 0.8297\n",
      "Epoch 7/20\n",
      "1381/1381 [==============================] - 27s 20ms/step - loss: 0.0906 - accuracy: 0.9765 - val_loss: 0.9546 - val_accuracy: 0.8258\n",
      "Epoch 8/20\n",
      "1381/1381 [==============================] - 27s 20ms/step - loss: 0.0839 - accuracy: 0.9777 - val_loss: 0.9798 - val_accuracy: 0.8340\n",
      "Epoch 9/20\n",
      "1381/1381 [==============================] - 30s 22ms/step - loss: 0.0819 - accuracy: 0.9778 - val_loss: 0.8168 - val_accuracy: 0.8509\n",
      "Epoch 10/20\n",
      "1381/1381 [==============================] - 33s 24ms/step - loss: 0.0801 - accuracy: 0.9786 - val_loss: 1.0471 - val_accuracy: 0.8103\n",
      "Epoch 11/20\n",
      "1381/1381 [==============================] - 28s 20ms/step - loss: 0.0753 - accuracy: 0.9796 - val_loss: 0.9719 - val_accuracy: 0.8440\n",
      "Epoch 12/20\n",
      "1381/1381 [==============================] - 27s 20ms/step - loss: 0.0675 - accuracy: 0.9823 - val_loss: 0.9930 - val_accuracy: 0.8407\n",
      "Epoch 13/20\n",
      "1381/1381 [==============================] - 27s 20ms/step - loss: 0.0746 - accuracy: 0.9795 - val_loss: 1.0693 - val_accuracy: 0.8244\n",
      "Epoch 14/20\n",
      "1381/1381 [==============================] - 27s 20ms/step - loss: 0.0640 - accuracy: 0.9830 - val_loss: 1.1255 - val_accuracy: 0.8358\n",
      "Epoch 15/20\n",
      "1381/1381 [==============================] - 28s 20ms/step - loss: 0.0665 - accuracy: 0.9817 - val_loss: 1.2195 - val_accuracy: 0.8262\n",
      "Epoch 16/20\n",
      "1381/1381 [==============================] - 28s 20ms/step - loss: 0.0662 - accuracy: 0.9818 - val_loss: 0.9420 - val_accuracy: 0.8431\n",
      "Epoch 17/20\n",
      "1381/1381 [==============================] - 27s 20ms/step - loss: 0.0591 - accuracy: 0.9840 - val_loss: 1.0919 - val_accuracy: 0.8368\n",
      "Epoch 18/20\n",
      "1381/1381 [==============================] - 27s 20ms/step - loss: 0.0581 - accuracy: 0.9836 - val_loss: 1.1081 - val_accuracy: 0.8266\n",
      "Epoch 19/20\n",
      "1381/1381 [==============================] - 27s 20ms/step - loss: 0.0612 - accuracy: 0.9834 - val_loss: 1.0406 - val_accuracy: 0.8507\n",
      "Epoch 20/20\n",
      "1381/1381 [==============================] - 28s 20ms/step - loss: 0.0568 - accuracy: 0.9842 - val_loss: 1.1225 - val_accuracy: 0.8368\n",
      "384/384 [==============================] - 2s 5ms/step - loss: 0.2872 - accuracy: 0.9448\n",
      "Epoch 1/20\n",
      "1381/1381 [==============================] - 28s 20ms/step - loss: 0.0966 - accuracy: 0.9757 - val_loss: 1.0226 - val_accuracy: 0.8330\n",
      "Epoch 2/20\n",
      "1381/1381 [==============================] - 30s 21ms/step - loss: 0.0727 - accuracy: 0.9795 - val_loss: 1.0639 - val_accuracy: 0.8293\n",
      "Epoch 3/20\n",
      "1381/1381 [==============================] - 27s 19ms/step - loss: 0.0623 - accuracy: 0.9831 - val_loss: 1.1373 - val_accuracy: 0.8350\n",
      "Epoch 4/20\n",
      "1381/1381 [==============================] - 28s 20ms/step - loss: 0.0632 - accuracy: 0.9824 - val_loss: 1.1559 - val_accuracy: 0.8328\n",
      "Epoch 5/20\n",
      "1381/1381 [==============================] - 27s 20ms/step - loss: 0.0577 - accuracy: 0.9841 - val_loss: 1.1846 - val_accuracy: 0.8344\n",
      "Epoch 6/20\n",
      "1381/1381 [==============================] - 27s 19ms/step - loss: 0.0587 - accuracy: 0.9833 - val_loss: 1.2227 - val_accuracy: 0.8311\n",
      "Epoch 7/20\n",
      "1381/1381 [==============================] - 27s 19ms/step - loss: 0.0550 - accuracy: 0.9847 - val_loss: 1.1199 - val_accuracy: 0.8421\n",
      "Epoch 8/20\n",
      "1381/1381 [==============================] - 28s 20ms/step - loss: 0.0534 - accuracy: 0.9852 - val_loss: 1.1729 - val_accuracy: 0.8401\n",
      "Epoch 9/20\n",
      "1381/1381 [==============================] - 30s 22ms/step - loss: 0.0500 - accuracy: 0.9863 - val_loss: 1.3460 - val_accuracy: 0.8140\n",
      "Epoch 10/20\n",
      "1381/1381 [==============================] - 27s 20ms/step - loss: 0.0533 - accuracy: 0.9849 - val_loss: 1.3791 - val_accuracy: 0.8258\n",
      "Epoch 11/20\n",
      "1381/1381 [==============================] - 27s 20ms/step - loss: 0.0460 - accuracy: 0.9876 - val_loss: 1.4969 - val_accuracy: 0.7993\n",
      "Epoch 12/20\n",
      "1381/1381 [==============================] - 27s 20ms/step - loss: 0.0519 - accuracy: 0.9848 - val_loss: 1.1959 - val_accuracy: 0.8317\n",
      "Epoch 13/20\n",
      "1381/1381 [==============================] - 28s 20ms/step - loss: 0.0459 - accuracy: 0.9870 - val_loss: 1.3662 - val_accuracy: 0.8207\n",
      "Epoch 14/20\n",
      "1381/1381 [==============================] - 27s 20ms/step - loss: 0.0435 - accuracy: 0.9879 - val_loss: 1.3130 - val_accuracy: 0.8230\n",
      "Epoch 15/20\n",
      "1381/1381 [==============================] - 27s 20ms/step - loss: 0.0512 - accuracy: 0.9854 - val_loss: 1.4478 - val_accuracy: 0.8232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20\n",
      "1381/1381 [==============================] - 27s 20ms/step - loss: 0.0433 - accuracy: 0.9877 - val_loss: 1.2710 - val_accuracy: 0.8338\n",
      "Epoch 17/20\n",
      "1381/1381 [==============================] - 29s 21ms/step - loss: 0.0420 - accuracy: 0.9882 - val_loss: 1.4413 - val_accuracy: 0.8187\n",
      "Epoch 18/20\n",
      "1381/1381 [==============================] - 29s 21ms/step - loss: 0.0462 - accuracy: 0.9869 - val_loss: 1.4747 - val_accuracy: 0.7969\n",
      "Epoch 19/20\n",
      "1381/1381 [==============================] - 27s 19ms/step - loss: 0.0429 - accuracy: 0.9881 - val_loss: 1.4116 - val_accuracy: 0.8205\n",
      "Epoch 20/20\n",
      "1381/1381 [==============================] - 27s 19ms/step - loss: 0.0408 - accuracy: 0.9886 - val_loss: 1.6515 - val_accuracy: 0.7888\n",
      "384/384 [==============================] - 2s 4ms/step - loss: 0.3041 - accuracy: 0.9419\n",
      "Epoch 1/20\n",
      "1381/1381 [==============================] - 27s 20ms/step - loss: 0.0781 - accuracy: 0.9785 - val_loss: 1.3823 - val_accuracy: 0.8246\n",
      "Epoch 2/20\n",
      "1381/1381 [==============================] - 27s 19ms/step - loss: 0.0524 - accuracy: 0.9857 - val_loss: 1.3968 - val_accuracy: 0.8207\n",
      "Epoch 3/20\n",
      "1381/1381 [==============================] - 27s 19ms/step - loss: 0.0497 - accuracy: 0.9862 - val_loss: 1.1793 - val_accuracy: 0.8478\n",
      "Epoch 4/20\n",
      "1381/1381 [==============================] - 27s 19ms/step - loss: 0.0482 - accuracy: 0.9861 - val_loss: 1.3045 - val_accuracy: 0.8378\n",
      "Epoch 5/20\n",
      "1381/1381 [==============================] - 27s 19ms/step - loss: 0.0499 - accuracy: 0.9859 - val_loss: 1.4378 - val_accuracy: 0.8220\n",
      "Epoch 6/20\n",
      "1381/1381 [==============================] - 27s 20ms/step - loss: 0.0435 - accuracy: 0.9881 - val_loss: 1.5678 - val_accuracy: 0.8085\n",
      "Epoch 7/20\n",
      "1381/1381 [==============================] - 27s 19ms/step - loss: 0.0524 - accuracy: 0.9847 - val_loss: 1.4673 - val_accuracy: 0.8189\n",
      "Epoch 8/20\n",
      "1381/1381 [==============================] - 27s 19ms/step - loss: 0.0427 - accuracy: 0.9880 - val_loss: 1.3494 - val_accuracy: 0.8383\n",
      "Epoch 9/20\n",
      "1381/1381 [==============================] - 27s 20ms/step - loss: 0.0395 - accuracy: 0.9889 - val_loss: 1.5136 - val_accuracy: 0.8118\n",
      "Epoch 10/20\n",
      "1381/1381 [==============================] - 27s 20ms/step - loss: 0.0435 - accuracy: 0.9880 - val_loss: 1.4093 - val_accuracy: 0.8311\n",
      "Epoch 11/20\n",
      "1381/1381 [==============================] - 27s 20ms/step - loss: 0.0409 - accuracy: 0.9889 - val_loss: 1.3998 - val_accuracy: 0.8419\n",
      "Epoch 12/20\n",
      "1381/1381 [==============================] - 27s 20ms/step - loss: 0.0448 - accuracy: 0.9870 - val_loss: 1.5422 - val_accuracy: 0.8207\n",
      "Epoch 13/20\n",
      "1381/1381 [==============================] - 27s 20ms/step - loss: 0.0363 - accuracy: 0.9899 - val_loss: 1.5159 - val_accuracy: 0.8132\n",
      "Epoch 14/20\n",
      "1381/1381 [==============================] - 27s 20ms/step - loss: 0.0425 - accuracy: 0.9876 - val_loss: 1.6887 - val_accuracy: 0.8044\n",
      "Epoch 15/20\n",
      "1381/1381 [==============================] - 28s 21ms/step - loss: 0.0408 - accuracy: 0.9884 - val_loss: 1.5427 - val_accuracy: 0.8273\n",
      "Epoch 16/20\n",
      "1381/1381 [==============================] - 27s 20ms/step - loss: 0.0434 - accuracy: 0.9870 - val_loss: 1.6241 - val_accuracy: 0.8146\n",
      "Epoch 17/20\n",
      "1381/1381 [==============================] - 27s 20ms/step - loss: 0.0364 - accuracy: 0.9899 - val_loss: 1.5420 - val_accuracy: 0.8262\n",
      "Epoch 18/20\n",
      "1381/1381 [==============================] - 27s 20ms/step - loss: 0.0355 - accuracy: 0.9905 - val_loss: 1.6094 - val_accuracy: 0.8087\n",
      "Epoch 19/20\n",
      "1381/1381 [==============================] - 28s 20ms/step - loss: 0.0427 - accuracy: 0.9883 - val_loss: 1.5066 - val_accuracy: 0.8317\n",
      "Epoch 20/20\n",
      "1381/1381 [==============================] - 27s 20ms/step - loss: 0.0405 - accuracy: 0.9881 - val_loss: 1.5602 - val_accuracy: 0.8228\n",
      "384/384 [==============================] - 2s 4ms/step - loss: 0.2333 - accuracy: 0.9578\n",
      "Epoch 1/20\n",
      "1381/1381 [==============================] - 27s 20ms/step - loss: 0.0628 - accuracy: 0.9826 - val_loss: 1.6651 - val_accuracy: 0.8065\n",
      "Epoch 2/20\n",
      "1381/1381 [==============================] - 27s 20ms/step - loss: 0.0444 - accuracy: 0.9876 - val_loss: 1.5029 - val_accuracy: 0.8364\n",
      "Epoch 3/20\n",
      "1381/1381 [==============================] - 28s 20ms/step - loss: 0.0405 - accuracy: 0.9891 - val_loss: 1.2833 - val_accuracy: 0.8368\n",
      "Epoch 4/20\n",
      "1381/1381 [==============================] - 29s 21ms/step - loss: 0.0402 - accuracy: 0.9886 - val_loss: 1.6397 - val_accuracy: 0.8077\n",
      "Epoch 5/20\n",
      "1381/1381 [==============================] - 28s 20ms/step - loss: 0.0419 - accuracy: 0.9881 - val_loss: 1.4723 - val_accuracy: 0.8326\n",
      "Epoch 6/20\n",
      "1381/1381 [==============================] - 27s 20ms/step - loss: 0.0371 - accuracy: 0.9896 - val_loss: 1.4660 - val_accuracy: 0.8232\n",
      "Epoch 7/20\n",
      "1381/1381 [==============================] - 27s 20ms/step - loss: 0.0452 - accuracy: 0.9870 - val_loss: 1.5172 - val_accuracy: 0.8315\n",
      "Epoch 8/20\n",
      "1381/1381 [==============================] - 28s 20ms/step - loss: 0.0373 - accuracy: 0.9894 - val_loss: 1.4863 - val_accuracy: 0.8146\n",
      "Epoch 9/20\n",
      "1381/1381 [==============================] - 27s 20ms/step - loss: 0.0384 - accuracy: 0.9884 - val_loss: 1.5068 - val_accuracy: 0.8342\n",
      "Epoch 10/20\n",
      "1381/1381 [==============================] - 27s 19ms/step - loss: 0.0358 - accuracy: 0.9897 - val_loss: 1.5865 - val_accuracy: 0.8262\n",
      "Epoch 11/20\n",
      "1381/1381 [==============================] - 27s 19ms/step - loss: 0.0424 - accuracy: 0.9881 - val_loss: 1.6603 - val_accuracy: 0.8216\n",
      "Epoch 12/20\n",
      "1381/1381 [==============================] - 27s 20ms/step - loss: 0.0356 - accuracy: 0.9900 - val_loss: 1.5007 - val_accuracy: 0.8281\n",
      "Epoch 13/20\n",
      "1381/1381 [==============================] - 28s 20ms/step - loss: 0.0351 - accuracy: 0.9903 - val_loss: 1.6676 - val_accuracy: 0.8211\n",
      "Epoch 14/20\n",
      "1381/1381 [==============================] - 27s 19ms/step - loss: 0.0358 - accuracy: 0.9901 - val_loss: 1.3636 - val_accuracy: 0.8501\n",
      "Epoch 15/20\n",
      "1381/1381 [==============================] - 27s 20ms/step - loss: 0.0387 - accuracy: 0.9889 - val_loss: 1.6138 - val_accuracy: 0.8279\n",
      "Epoch 16/20\n",
      "1381/1381 [==============================] - 27s 20ms/step - loss: 0.0357 - accuracy: 0.9899 - val_loss: 1.5696 - val_accuracy: 0.8344\n",
      "Epoch 17/20\n",
      "1381/1381 [==============================] - 27s 20ms/step - loss: 0.0328 - accuracy: 0.9910 - val_loss: 1.9304 - val_accuracy: 0.8028\n",
      "Epoch 18/20\n",
      "1381/1381 [==============================] - 27s 20ms/step - loss: 0.0378 - accuracy: 0.9891 - val_loss: 1.4477 - val_accuracy: 0.8411\n",
      "Epoch 19/20\n",
      "1381/1381 [==============================] - 27s 20ms/step - loss: 0.0328 - accuracy: 0.9909 - val_loss: 1.6521 - val_accuracy: 0.8311\n",
      "Epoch 20/20\n",
      "1381/1381 [==============================] - 27s 20ms/step - loss: 0.0346 - accuracy: 0.9899 - val_loss: 1.6142 - val_accuracy: 0.8383\n",
      "384/384 [==============================] - 2s 4ms/step - loss: 0.2236 - accuracy: 0.9597\n"
     ]
    }
   ],
   "source": [
    "# evaluating model performance with cv and recording the accuracy in the dictionary 'cv_results'\n",
    "\n",
    "cv_results = {}\n",
    "for i in range(5):\n",
    "\n",
    "    # train the model\n",
    "    history = model.fit(cv[i][0], cv[i][2],validation_split = 0.1, epochs=20, verbose=1)\n",
    "\n",
    "    # evaluate the model\n",
    "    loss, accuracy = model.evaluate(cv[i][1], cv[i][3], verbose=1)\n",
    "    cv_results[i] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.9231521487236023,\n",
       " 1: 0.9448292851448059,\n",
       " 2: 0.9418955445289612,\n",
       " 3: 0.9577866792678833,\n",
       " 4: 0.9597424864768982}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Modeling - final model\n",
    "- Here we use all the data to train the lstm model so that its performance can be improved\n",
    "\n",
    "- <b> Note: please do not include your test dataset at this point<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 64, 100)           3438900   \n",
      "_________________________________________________________________\n",
      "masking_1 (Masking)          (None, 64, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                17024     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 31)                527       \n",
      "=================================================================\n",
      "Total params: 3,456,979\n",
      "Trainable params: 18,079\n",
      "Non-trainable params: 3,438,900\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "1726/1726 [==============================] - 38s 21ms/step - loss: 1.9739 - accuracy: 0.4955 - val_loss: 0.8097 - val_accuracy: 0.8026\n",
      "Epoch 2/20\n",
      "1726/1726 [==============================] - 35s 20ms/step - loss: 0.6722 - accuracy: 0.8290 - val_loss: 0.6564 - val_accuracy: 0.8442\n",
      "Epoch 3/20\n",
      "1726/1726 [==============================] - 34s 20ms/step - loss: 0.4522 - accuracy: 0.8835 - val_loss: 0.6539 - val_accuracy: 0.8439\n",
      "Epoch 4/20\n",
      "1726/1726 [==============================] - 35s 20ms/step - loss: 0.3457 - accuracy: 0.9115 - val_loss: 0.7021 - val_accuracy: 0.8186\n",
      "Epoch 5/20\n",
      "1726/1726 [==============================] - 36s 21ms/step - loss: 0.2892 - accuracy: 0.9253 - val_loss: 0.7000 - val_accuracy: 0.8186\n",
      "Epoch 6/20\n",
      "1726/1726 [==============================] - 36s 21ms/step - loss: 0.2451 - accuracy: 0.9356 - val_loss: 0.7933 - val_accuracy: 0.8152\n",
      "Epoch 7/20\n",
      "1726/1726 [==============================] - 39s 22ms/step - loss: 0.2240 - accuracy: 0.9412 - val_loss: 0.6715 - val_accuracy: 0.8449\n",
      "Epoch 8/20\n",
      "1726/1726 [==============================] - 41s 24ms/step - loss: 0.1954 - accuracy: 0.9491 - val_loss: 0.7351 - val_accuracy: 0.8228\n",
      "Epoch 9/20\n",
      "1726/1726 [==============================] - 38s 22ms/step - loss: 0.1817 - accuracy: 0.9521 - val_loss: 0.7834 - val_accuracy: 0.8230\n",
      "Epoch 10/20\n",
      "1726/1726 [==============================] - 38s 22ms/step - loss: 0.1714 - accuracy: 0.9554 - val_loss: 0.7597 - val_accuracy: 0.8391\n",
      "Epoch 11/20\n",
      "1726/1726 [==============================] - 38s 22ms/step - loss: 0.1515 - accuracy: 0.9603 - val_loss: 0.7469 - val_accuracy: 0.8295\n",
      "Epoch 12/20\n",
      "1726/1726 [==============================] - 38s 22ms/step - loss: 0.1463 - accuracy: 0.9623 - val_loss: 0.7920 - val_accuracy: 0.8341\n",
      "Epoch 13/20\n",
      "1726/1726 [==============================] - 38s 22ms/step - loss: 0.1295 - accuracy: 0.9656 - val_loss: 0.7803 - val_accuracy: 0.8422\n",
      "Epoch 14/20\n",
      "1726/1726 [==============================] - 39s 22ms/step - loss: 0.1275 - accuracy: 0.9655 - val_loss: 0.7030 - val_accuracy: 0.8611\n",
      "Epoch 15/20\n",
      "1726/1726 [==============================] - 38s 22ms/step - loss: 0.1193 - accuracy: 0.9673 - val_loss: 0.7895 - val_accuracy: 0.8333\n",
      "Epoch 16/20\n",
      "1726/1726 [==============================] - 38s 22ms/step - loss: 0.1101 - accuracy: 0.9702 - val_loss: 0.7392 - val_accuracy: 0.8478\n",
      "Epoch 17/20\n",
      "1726/1726 [==============================] - 39s 23ms/step - loss: 0.1031 - accuracy: 0.9724 - val_loss: 0.8408 - val_accuracy: 0.8269\n",
      "Epoch 18/20\n",
      "1726/1726 [==============================] - 39s 22ms/step - loss: 0.1008 - accuracy: 0.9724 - val_loss: 1.0096 - val_accuracy: 0.7924\n",
      "Epoch 19/20\n",
      "1726/1726 [==============================] - 38s 22ms/step - loss: 0.0959 - accuracy: 0.9744 - val_loss: 0.7860 - val_accuracy: 0.8582\n",
      "Epoch 20/20\n",
      "1726/1726 [==============================] - 41s 24ms/step - loss: 0.0890 - accuracy: 0.9751 - val_loss: 0.7700 - val_accuracy: 0.8603\n"
     ]
    }
   ],
   "source": [
    "# training the final model\n",
    "model = make_lstm_classification_model()\n",
    "history = model.fit(padded_docs, labels,validation_split = 0.1, epochs=20, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Evaluate the model using your test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Please replace X_test with your transformed predictor variable\n",
    "# Please replace y_test with your transformed target variable\n",
    "# And run the following code\n",
    "\n",
    "#loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "#print('Accuracy: %f' % (accuracy*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Modeling",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
